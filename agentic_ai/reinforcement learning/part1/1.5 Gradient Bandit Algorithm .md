# Gradient Bandit Algorithm 

# Gradient Bandit Algorithm (Ultra-Simplified with Real-Life Examples)

---

## 1ï¸âƒ£ Real-Life Examples / Applications

### 1. Online Ads
- Many ads to show to users  
- You want to show ads that get clicks  
- Gradient Bandit adjusts how much you â€œpreferâ€ each ad based on clicks  

### 2. Movie / Content Recommendation
- Streaming platforms recommend shows  
- If users like a recommended show â†’ increase preference  
- If they skip â†’ decrease preference  
- Keeps learning what users like most  

### 3. Snack / Product Choices
- Imagine a vending machine with many snacks  
- Every time you pick one, you rate it  
- Preference scores slowly adjust so you pick the ones you like more often  

### 4. Game Strategies
- Video games or AI players try moves  
- Reward = winning points  
- Gradient Bandit increases preference for moves that work  

---

## 2ï¸âƒ£ What Is Gradient Bandit? (Ultra-Simple)
- It doesnâ€™t learn â€œhow much rewardâ€ an action gives, it learns â€œhow much I like it.â€  
- Each action has a **preference score H(a)**  
- Higher H â†’ more likely to pick  
- Lower H â†’ less likely  

---

## 3ï¸âƒ£ How It Works Step by Step

### Start
- Give all actions H = 0 (neutral)  

### Pick an Action
- Use probabilities based on H (soft choice)  

### Get Reward
- Compare reward with average reward so far  

### Update Preferences
- Reward better than average â†’ increase chosen action preference, decrease others  
- Reward worse than average â†’ decrease chosen action preference, increase others  

### Repeat
- Preferences gradually favor better actions  
- Some exploration remains  

---

## 4ï¸âƒ£ Tiny Real-Life Analogy

ðŸŽ’ **Choosing Snacks at School**  
- Try chocolate, chips, cookies  
- Rate how much you liked each  
- Next time, pick chocolate more often if liked  
- Sometimes try chips or cookies to see if you like them now  

---

## 5ï¸âƒ£ One-Line Memory Trick ðŸ§ 
> Gradient Bandit = â€œAdjust your like score up or down depending on how good it was.â€


---

## 1. Real-Life Applications

Gradient Bandit algorithms are widely used in **multi-armed bandit problems**, where we want to select actions to maximize rewards. Examples include:

| Domain | Example |
|--------|---------|
| Online Advertising | Show ads that maximize click-through rates. |
| Recommendation Systems | Recommend movies or products that users like most. |
| Game AI | Learn which moves lead to winning points in strategy games. |
| Vending / Retail | Adjust product placement or offers based on customer preference. |

**Key Insight:**  
> Gradient Bandit doesnâ€™t estimate the value of each action, it **learns the preference for each action**.

---

## 2. Gradient Bandit Concept
# Gradient Bandit Algorithm â€“ Theory and Formulas

---

# Gradient Bandit Algorithm â€“ Preferences and Updates

---

## Preferences and Probabilities

Each action \(a\) has a **preference**:

\[
H(a)
\]

The probability of choosing an action is calculated using the **Softmax function**:

\[
P(a) = \frac{e^{H(a)}}{\sum_{b} e^{H(b)}} 
\]

Where:  
- \(H(a)\) = preference of action \(a\)  
- \(P(a)\) = probability of selecting action \(a\)  

---

## Updating Preferences

After taking action \(a\) and receiving reward \(R\), preferences are updated as follows:

\[
H(a) \gets H(a) + \alpha (R - \bar{R}) (1 - P(a))
\]

\[
H(b) \gets H(b) - \alpha (R - \bar{R}) P(b), \quad b \neq a
\]

Where:  
- \(\alpha\) = learning rate  
- \(\bar{R}\) = average reward so far  

---

## Interpretation

- **Reward better than average** â†’ increase preference for the chosen action  
- **Reward worse than average** â†’ decrease preference for the chosen action  

---

## Key Points

- Gradient Bandit **learns preferences**, not Q-values.  
- Uses **Softmax** to convert preferences into probabilities.  
- Gradual updates encourage **exploration and exploitation**.  
- Useful in **non-stationary environments**.


## Key Points

- Gradient Bandit **learns preferences**, not action values  
- Probabilities are **softmaxed** based on preferences  
- Gradual updates ensure **exploration and exploitation**  
- Works well for **non-stationary environments**
  

**Interpretation:**  
- Reward better than average â†’ increase preference for chosen action  
- Reward worse than average â†’ decrease preference for chosen action  

---

## 3. Step-by-Step Example

### Problem Setup

- Actions: A, B, C  
- Rewards are stochastic:  
  - A â†’ 1  
  - B â†’ 0.5  
  - C â†’ 0.2  
- Learning rate: Î± = 0.1  

### Step 1: Initialize Preferences

| Action | H(a) | Probability P(a) |
|--------|------|-----------------|
| A      | 0    | 0.33            |
| B      | 0    | 0.33            |
| C      | 0    | 0.33            |

---

### Step 2: Choose an Action

- Randomly pick **A** (based on probabilities)

### Step 3: Observe Reward

- Reward R = 1  

### Step 4: Update Preferences

- Average reward so far: \( \bar{R} = 0.5 \)  
- Update H(A), H(B), H(C) using formula above  

| Action | H(a) new | P(a) new |
|--------|-----------|----------|
| A      | 0.05      | 0.34     |
| B      | -0.025    | 0.33     |
| C      | -0.025    | 0.33     |

---

### Step 5: Repeat

- Next iteration, pick action **probabilistically** using updated P(a)  
- Continue updating preferences after each reward  

---

## 4. Python Implementation (Beginner-Friendly)

```python
import numpy as np

# Parameters
actions = ['A', 'B', 'C']
H = np.zeros(len(actions))          # Preferences
alpha = 0.1                         # Learning rate
average_reward = 0
reward_count = 0

# Simulated true rewards for actions
true_rewards = [1.0, 0.5, 0.2]

# Softmax function
def softmax(H):
    eH = np.exp(H)
    return eH / np.sum(eH)

# Run Gradient Bandit for 10 steps
for step in range(10):
    probs = softmax(H)
    action_index = np.random.choice(len(actions), p=probs)
    
    # Get reward
    reward = np.random.normal(true_rewards[action_index], 0.1)
    
    # Update average reward
    reward_count += 1
    average_reward += (reward - average_reward) / reward_count
    
    # Update preferences
    for i in range(len(actions)):
        if i == action_index:
            H[i] += alpha * (reward - average_reward) * (1 - probs[i])
        else:
            H[i] -= alpha * (reward - average_reward) * probs[i]
    
    print(f"Step {step+1}: Action {actions[action_index]}, Reward {reward:.2f}, H = {H.round(2)}, P = {softmax(H).round(2)}")
