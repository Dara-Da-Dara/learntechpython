# Reinforcement Learning (RL) Concepts and Algorithms

| **S.No** | **Concept / Algorithm**              | **Definition**                                                                                                                                  | **Year Introduced** | **Use Case / Real-World Example**                                           | **Popularity / Application**                                             |
| -------- | ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| 1        | Reinforcement Learning (RL)          | RL is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. | 1980s               | Video game AI, Robotics, Recommendation Systems, Autonomous Vehicles        | Widely used in AI, robotics, and recommendation engines                  |
| 2        | Optimistic Initial Values            | Initializing value estimates to higher-than-expected values to encourage exploration in RL.                                                     | 1989                | Multi-Armed Bandit problems; encourages exploration in unknown environments | Popular in bandit problems and exploration-heavy environments            |
| 3        | Gradient Bandit Algorithm            | A method in RL where preferences for actions are updated using gradient ascent on expected reward.                                              | 1992                | Online ad placement, Adaptive game strategies                               | Useful in action selection problems and non-stationary environments      |
| 4        | Bellman Equation                     | A recursive equation for computing the value of a state under a given policy.                                                                   | 1957                | Dynamic programming, MDP solutions                                          | Fundamental to value-based RL methods                                    |
| 5        | Bellman Optimality Equation          | Recursive equation used to find the optimal policy by maximizing rewards over all actions.                                                      | 1957                | Finding optimal strategies in games, planning tasks                         | Key in dynamic programming and Q-learning                                |
| 6        | TD(0)                                | Temporal Difference learning that updates value estimates after each step using the observed reward and estimate of the next state.             | 1988                | Predicting stock prices, simple gridworld navigation                        | Basic RL algorithm, easy to implement, used in many online RL tasks      |
| 7        | TD(\u03BB)                           | Extension of TD(0) that uses eligibility traces to update value estimates over multiple steps.                                                  | 1988                | Robot navigation, control tasks                                             | Improved learning efficiency over TD(0), widely used in control RL tasks |
| 8        | Q-Learning                           | Off-policy model-free RL algorithm that learns the value of action-state pairs to find the optimal policy.                                      | 1989                | Game AI, Self-driving cars, Resource allocation                             | One of the most popular RL algorithms due to simplicity and convergence  |
| 9        | SARSA                                | On-policy RL algorithm that updates action-state values based on the agent's actual behavior.                                                   | 1994                | Adaptive traffic signal control, online games                               | Useful when following the current policy matters, safer learning         |
| 10       | Deep Q-Network (DQN)                 | Combines Q-learning with deep neural networks to handle high-dimensional state spaces.                                                          | 2015                | Atari game playing, robotics                                                | Popular in deep RL, first major success of deep RL                       |
| 11       | Double DQN                           | Improvement over DQN to reduce overestimation bias in Q-value estimation.                                                                       | 2016                | Autonomous driving, strategy games                                          | More stable learning in high-dimensional RL tasks                        |
| 12       | Policy Parameterization              | Approach in RL where policies are represented with parameters (e.g., neural networks) and optimized directly.                                   | 1999                | Robotics, continuous control                                                | Basis for policy-gradient methods                                        |
| 13       | REINFORCE Algorithm                  | A policy-gradient method that updates policy parameters in the direction of expected reward gradient.                                           | 1992                | Robot arm manipulation, games                                               | Simple and foundational policy-gradient RL method                        |
| 14       | Dyna Architecture                    | Integrates learning, planning, and acting in RL by combining model-free and model-based approaches.                                             | 1991                | Robot learning, planning in dynamic environments                            | Efficient RL; reduces interaction with real environment                  |
| 15       | Generalization in RL                 | Ability of an RL agent to perform well in unseen environments or states.                                                                        | 2010s               | Game AI transferring skills, robotics                                       | Difficult due to sparse rewards and complex environments                 |
| 16       | DAgger (Dataset Aggregation)         | Imitation learning algorithm that collects expert demonstrations to improve policy iteratively.                                                 | 2011                | Autonomous driving, robotic manipulation                                    | Useful for imitation learning where reward is sparse or hard             |
| 17       | Inverse Reinforcement Learning (IRL) | Learns the reward function by observing expert behavior.                                                                                        | 2000                | Self-driving cars, behavioral cloning                                       | Widely used in robotics and autonomous systems to mimic experts          |
| 18       | Policy Gradient in MARL              | Method to optimize policies in Multi-Agent RL using gradient-based updates.                                                                     | 2000s               | Multi-player games, cooperative robotics                                    | Useful in multi-agent environments where agents learn collaboratively    |

## Key Notes for Understanding

* **Value (V)**: Expected cumulative reward from a state.
* **Action (a)**: Choice made by the agent.
* **Policy (Ï€)**: Strategy mapping states to actions.
* **Reward (R)**: Feedback signal after performing an action.
* **State (s)**: Current situation of the environment.
* **Eligibility Trace**: Memory of past states/actions for TD(\u03BB).
