{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piqt7pOWy6Rj",
        "outputId": "da91cfd0-0a1f-4b21-f1eb-b9d0ff17be91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Action B, Reward 0.50, H = [0. 0. 0.], P = [0.33 0.33 0.33]\n",
            "Step 2: Action C, Reward 0.39, H = [ 0.  0. -0.], P = [0.33 0.33 0.33]\n",
            "Step 3: Action C, Reward 0.23, H = [ 0.01  0.01 -0.01], P = [0.34 0.34 0.33]\n",
            "Step 4: Action C, Reward 0.15, H = [ 0.01  0.01 -0.02], P = [0.34 0.34 0.33]\n",
            "Step 5: Action C, Reward 0.01, H = [ 0.02  0.02 -0.04], P = [0.34 0.34 0.32]\n",
            "Step 6: Action C, Reward 0.33, H = [ 0.02  0.02 -0.04], P = [0.34 0.34 0.32]\n",
            "Step 7: Action A, Reward 0.95, H = [ 0.06 -0.   -0.06], P = [0.35 0.33 0.31]\n",
            "Step 8: Action B, Reward 0.58, H = [ 0.05  0.01 -0.06], P = [0.35 0.34 0.31]\n",
            "Step 9: Action C, Reward 0.22, H = [ 0.06  0.02 -0.07], P = [0.35 0.34 0.31]\n",
            "Step 10: Action A, Reward 0.95, H = [ 0.09 -0.   -0.09], P = [0.36 0.33 0.3 ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "actions = ['A', 'B', 'C']\n",
        "H = np.zeros(len(actions))          # Preferences\n",
        "alpha = 0.1                         # Learning rate\n",
        "average_reward = 0\n",
        "reward_count = 0\n",
        "\n",
        "# Simulated true rewards for actions\n",
        "true_rewards = [1.0, 0.5, 0.2]\n",
        "\n",
        "# Softmax function\n",
        "def softmax(H):\n",
        "    eH = np.exp(H)\n",
        "    return eH / np.sum(eH)\n",
        "\n",
        "# Run Gradient Bandit for 10 steps\n",
        "for step in range(10):\n",
        "    probs = softmax(H)\n",
        "    action_index = np.random.choice(len(actions), p=probs)\n",
        "\n",
        "    # Get reward\n",
        "    reward = np.random.normal(true_rewards[action_index], 0.1)\n",
        "\n",
        "    # Update average reward\n",
        "    reward_count += 1\n",
        "    average_reward += (reward - average_reward) / reward_count\n",
        "\n",
        "    # Update preferences\n",
        "    for i in range(len(actions)):\n",
        "        if i == action_index:\n",
        "            H[i] += alpha * (reward - average_reward) * (1 - probs[i])\n",
        "        else:\n",
        "            H[i] -= alpha * (reward - average_reward) * probs[i]\n",
        "\n",
        "    print(f\"Step {step+1}: Action {actions[action_index]}, Reward {reward:.2f}, H = {H.round(2)}, P = {softmax(H).round(2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NOVPK03y7P3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}