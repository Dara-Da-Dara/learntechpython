# **UNIT II – Techniques and Best Practices in Prompt Engineering**

---

## **1. Overview of Prompting Techniques**

After understanding the fundamentals of prompt engineering in UNIT I, this unit focuses on **practical prompting techniques and best practices** that significantly improve the quality, consistency, and reliability of Large Language Model (LLM) outputs. These techniques help users control *how much guidance* the model receives and *how it reasons* before generating an answer.

---

## **2. Zero-Shot Prompting**

### **2.1 Concept**
Zero-shot prompting refers to giving a task to the model **without providing any examples**. The model relies entirely on its pre-trained knowledge.

### **2.2 When to Use**
- Simple or common tasks
- General knowledge questions
- Quick explanations

### **2.3 Example**

**Prompt**
```text
Explain supervised learning.
```

**Output**
```text
Supervised learning is a machine learning approach where models are trained using labeled data to make predictions or classifications.
```

### **2.4 Limitation**
- May lack depth or structure for complex tasks

---

## **3. One-Shot Prompting**

### **3.1 Concept**
One-shot prompting provides **a single example** to guide the model toward the expected output style or format.

### **3.2 Example**

**Prompt**
```text
Convert the sentence into passive voice.
Example:
Active: The teacher explained the concept.
Passive: The concept was explained by the teacher.

Now convert:
Active: The model generated the response.
```

**Output**
```text
Passive: The response was generated by the model.
```

---

## **4. Few-Shot Prompting**

### **4.1 Concept**
Few-shot prompting includes **multiple examples** to clearly establish a pattern for the model to follow.

### **4.2 When to Use**
- Domain-specific tasks
- Format-sensitive outputs
- Classification or transformation problems

### **4.3 Example**

**Prompt**
```text
Classify the sentiment of the text.
Text: I love this product → Positive
Text: This service is terrible → Negative
Text: The experience was average → Neutral
Text: The app is very slow →
```

**Output**
```text
Negative
```

---

## **5. Chain-of-Thought (CoT) Prompting**

### **5.1 Concept**
Chain-of-Thought prompting encourages the model to **reason step by step** before producing the final answer.

### **5.2 Importance**
- Improves logical reasoning
- Reduces incorrect conclusions
- Useful for math and analytical problems

### **5.3 Example**

**Prompt**
```text
Solve the problem step by step.
If a shopkeeper buys an item for ₹200 and sells it for ₹250, what is the profit?
```

**Output**
```text
The cost price is ₹200.
The selling price is ₹250.
Profit = 250 − 200 = ₹50.
```

---

## **6. Role-Based (Persona) Prompting**

### **6.1 Concept**
Role prompting assigns a **specific persona or role** to the model to influence tone, depth, and perspective.

### **6.2 Example**

**Prompt**
```text
You are a career counselor.
Explain the importance of learning AI skills to undergraduate students.
```

**Output**
```text
Learning AI skills opens diverse career opportunities and enhances problem-solving abilities required in modern industries.
```

---

## **7. Best Practices for Prompt Design**

### **7.1 Use Clear Instructions**
Avoid vague language and clearly define the task.

### **7.2 Provide Context When Needed**
Context improves relevance and reduces ambiguity.

### **7.3 Apply Constraints**
Control length, format, and tone.

### **7.4 Specify Output Format**
Request tables, bullet points, or structured responses.

---

## **8. Advanced Strategies to Enhance Output Quality and Consistency**

- Use step-by-step instructions for complex tasks
- Break tasks using prompt chaining
- Ask the model to verify or summarize its own answer
- Combine role prompting with constraints

---

## **9. Prompt Chaining**

### **9.1 Concept**
Prompt chaining breaks a complex task into **multiple smaller prompts**, where the output of one prompt becomes the input of the next.

### **9.2 Example**

**Prompt 1**
```text
Summarize the following article in 100 words.
```

**Prompt 2**
```text
Convert the summary into 5 key bullet points.
```

---

## **10. Common Mistakes in Applying Techniques**

- Overloading prompts with multiple tasks
- Using few-shot examples that are inconsistent
- Applying CoT unnecessarily for simple tasks
- Ignoring output format specification

---

## **UNIT II – Learning Outcomes**

After completing UNIT II, students will be able to:
- Differentiate between zero-shot, one-shot, and few-shot prompting
- Apply Chain-of-Thought prompting for reasoning tasks
- Use role-based prompting to control model behavior
- Implement best practices for clear and consistent prompts
- Design multi-step solutions using prompt chaining
